{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juan026/EVA3D/blob/main/ComputerGraphics_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#EVA3D: Compositional 3D Human Generation from 2D Image Collections\n",
        "\n",
        "*Hong, F., Chen, Z., Lan, Y., Pan, L., & Liu, Z. (2022). Eva3d: Compositional 3d human generation from 2d image collections. arXiv preprint arXiv:2210.04888.*\n",
        "\n",
        "Juan Antonio Del Hoyo Ontiveros\n",
        "\n",
        "En este se realiza un revisión y adaptación del código con el fin de evaluarlo.\n",
        "Este articulo propone un modelo de tipo red generativa antagónica GAN el cual se entrenó a partir de imágenes 2D para crear lo que se conoce como inversión de gráficos, es decir, pasar de representación 2D a 3D. En el método propuesto los autores aplican una división del cuerpo humano en partes locales,  de esta forma pueden enfocar el aprendizaje en las distintas áreas donde se identifican las articulaciones del cuerpo humano. Finalmente, se logra representar las partes articuladas de manera más precisa.\n",
        "\n",
        "\n",
        "Este demo utiliza el modelo pre-entrenado en la base de datos DeepFashion.\n",
        "\n",
        "\n",
        "Enlaces:\n",
        "\n",
        "Original Github Repo: https://github.com/hongfz16/EVA3D\n",
        "\n",
        "Artículo arXiv Link: https://arxiv.org/abs/2210.04888\n"
      ],
      "metadata": {
        "id": "dZ8b7nDOoy1-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gj9jUt7nZMBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code used for Computer Graphics Project \n",
        "# Setup Everything\n",
        "\n",
        "# Importamos la copia del repositorio e instalamos todas las librerias requeridas por el proyecto\n",
        "!nvidia-smi\n",
        "!git clone https://github.com/juan026/EVA3D.git\n",
        "\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install fvcore iopath\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "!pip install -r EVA3D/requirements.txt"
      ],
      "metadata": {
        "id": "6cDCtHPy8z39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Models\n",
        "\n",
        "# Se descargan los modelos preentrenados \n",
        "%cd EVA3D\n",
        "from download_models import download_pretrained_models\n",
        "download_pretrained_models()"
      ],
      "metadata": {
        "id": "8HtVIDlp3ldz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bGbOgk-h4_7M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iy151ysWxMO"
      },
      "source": [
        "Register and download SMPL models [here](https://smpl.is.tue.mpg.de/). Put the downloaded models in the folder smpl_models. Only the neutral one is needed. The folder structure should look like\n",
        "\n",
        "```\n",
        "./\n",
        "├── ...\n",
        "└── smpl_models/\n",
        "    ├── smpl/\n",
        "        └── SMPL_NEUTRAL.pkl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download SMPL NEUTRAL model\n",
        "!ls\n",
        "# Descagamos el modelo para la generación de cuerpos humanos en 3D\n",
        "!mkdir smpl_models\n",
        "%cd smpl_models\n",
        "!mkdir smpl\n",
        "%cd smpl\n",
        "!ls\n",
        "!gdown 1DTiAXbVnCRopZ-LObAqKnT-9rv-aySc5\n",
        "!ls"
      ],
      "metadata": {
        "id": "-hhNZp3s_lFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download dataset for training\n",
        "%cd ../..\n",
        "!ls\n",
        "# Se descarga el conjunto de datos Deep Fashion\n",
        "!python download_datasets.py"
      ],
      "metadata": {
        "id": "-IIxFWdn7ocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train a new model using deepfashion dataset\n",
        "%cd EVA3D\n",
        "!python train_deepfashion.py --batch 6 --chunk 1 --expname train_deepfashion_512x256 --dataset_path datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --r1 300 --voxhuman_name eva3d_deepfashion --random_flip --eikonal_lambda 0.5 --small_aug --iter 1000 --adjust_gamma --gamma_lb 20 --min_surf_lambda 1.5 --deltasdf --gaussian_weighted_sampler --sampler_std 15 --N_samples 14\n"
      ],
      "metadata": {
        "id": "t_m7GjsZbLsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Start the Genenration!\n",
        "# Ejecutamos la inferencia por defecto\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path demodataset --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "aMSFMTb5UOE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define images for evaluation\n",
        "import os\n",
        "# define images for evaluation\n",
        "# Se definen las rutas con las imagenes de ejemplos \n",
        "# seleccionados para evaluar el modelo\n",
        "root_dataset = \"/content/EVA3D/datasets/DeepFashion/images/\"\n",
        "\n",
        "testing_images= [\"MEN-Jackets_Vests-id_00005346-01_4_full.png\",\n",
        "                 \"WOMEN-Dresses-id_00006993-03_7_additional.png\",\n",
        "                 \"WOMEN-Blouses_Shirts-id_00001722-07_7_additional.png\"]\n",
        "\n",
        "\n",
        "img_testing1 = os.path.join(root_dataset, testing_images[0])\n",
        "img_testing2 = os.path.join(root_dataset, testing_images[1])\n",
        "img_testing3 = os.path.join(root_dataset, testing_images[2])"
      ],
      "metadata": {
        "id": "0CKkRC9iCmvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define code for transformation\n",
        "\n",
        "# Se importan las librerias de OpenCV para aplicar operaciones de \n",
        "# transformaciones a las imágenes.\n",
        "# Run transformations\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QAZTzN7HCq1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Translation\n",
        "# Translation\n",
        "# Se aplica la transformación de traslación en los ejes X,Y con 200 y 150 \n",
        "# pixeles respectivamente.\n",
        "image = cv2.imread(img_testing1)\n",
        "ancho = image.shape[1] #columnas\n",
        "alto = image.shape[0] # filas\n",
        "\n",
        "M = np.float32([[1, 0,200],\n",
        "                [0, 1,150]])\n",
        "imageOut = cv2.warpAffine(image,M,(ancho,alto))\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "cv2.waitKey(0)\n",
        "# Se guarda la imagen para su posterior inferencia\n",
        "# en el modelo.\n",
        "output_img_path = img_testing1[:-4]+\"__translation.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "KeQQjk4oRnzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Translation\n",
        "# Ejecutamos la inferencia en la imagen anterior\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "ys1OzozJXkfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Rotation\n",
        "# Rotation\n",
        "# Se aplica la transformación de rotación usando como eje de rotación \n",
        "# el centro dividiendo por 2 su tamaño, después se rota 30 grados sobre el eje\n",
        "# y no se aplica ningún escalamiento.\n",
        "# pixeles respectivamente.\n",
        "image = cv2.imread(img_testing2)\n",
        "ancho = image.shape[1] #columnas\n",
        "alto = image.shape[0] # filas\n",
        "# Rotación\n",
        "M = cv2.getRotationMatrix2D((ancho//2,alto//2),30,1)\n",
        "imageOut = cv2.warpAffine(image,M,(ancho,alto))\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "# Se guarda la imagen para su posterior inferencia\n",
        "# en el modelo.\n",
        "output_img_path = img_testing2[:-4]+\"__rotation.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "BJFpKzLZSRwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Rotation\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "0YHXRC0hSwa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Scale\n",
        "image = cv2.imread(img_testing3)\n",
        "# Scale\n",
        "# Se aplica la transformación de escalamiento en tamaño\n",
        "# de 650x650 pixeles.\n",
        "# y no se aplica ningún escalamiento.\n",
        "imageOut = cv2.resize(image,(650,650), interpolation=cv2.INTER_CUBIC)\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "output_img_path = img_testing3[:-4]+\"__scale.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "# Se guarda la imagen para su posterior inferencia\n",
        "# en el modelo.\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "gM3HbyclTueR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Scale\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "EUMnipDVXKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusiones\n",
        "\n",
        "Se evalúa aplicando inferencias en el modelo propuesto por los autores del artículo.\n",
        "\n",
        "Se realizaron las siguientes transformaciones: Rotación, escalamiento y traslación en las imágenes.\n",
        "\n",
        "Las salidas demuestran que el modelo es capaz de recrear las representaciones 3D\n",
        "a partir de las observaciones 2D.\n",
        "\n",
        "Se requiere realizar más experimentación en las evaluaciones, cambio de parámetros con el fin de mejorar los resultados obtenidos.\n"
      ],
      "metadata": {
        "id": "iKNr45GKXoS7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIUYM9KlZEha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}