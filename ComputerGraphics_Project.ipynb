{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUf+DjT80cFHdksYPa+FMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juan026/EVA3D/blob/main/ComputerGraphics_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code used for Computer Graphics Project \n",
        "# Setup Everything\n",
        "\n",
        "!nvidia-smi\n",
        "!git clone https://github.com/juan026/EVA3D.git\n",
        "\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install fvcore iopath\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "!pip install -r EVA3D/requirements.txt"
      ],
      "metadata": {
        "id": "6cDCtHPy8z39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train a new model using deepfashion dataset\n",
        "!python train_deepfashion.py --batch 8 --chunk 1 --expname train_deepfashion_512x256 --dataset_path datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --r1 300 --voxhuman_name eva3d_deepfashion --random_flip --eikonal_lambda 0.5 --small_aug --iter 1000 --adjust_gamma --gamma_lb 20 --min_surf_lambda 1.5 --deltasdf --gaussian_weighted_sampler --sampler_std 15 --N_samples 14\n"
      ],
      "metadata": {
        "id": "1DQ4xgniTz5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Models\n",
        "%cd EVA3D\n",
        "from download_models import download_pretrained_models\n",
        "download_pretrained_models()"
      ],
      "metadata": {
        "id": "8HtVIDlp3ldz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bGbOgk-h4_7M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iy151ysWxMO"
      },
      "source": [
        "Register and download SMPL models [here](https://smpl.is.tue.mpg.de/). Put the downloaded models in the folder smpl_models. Only the neutral one is needed. The folder structure should look like\n",
        "\n",
        "```\n",
        "./\n",
        "├── ...\n",
        "└── smpl_models/\n",
        "    ├── smpl/\n",
        "        └── SMPL_NEUTRAL.pkl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download SMPL NEUTRAL model\n",
        "!ls\n",
        "\n",
        "!mkdir smpl_models\n",
        "%cd smpl_models\n",
        "!mkdir smpl\n",
        "%cd smpl\n",
        "!ls\n",
        "!gdown 1DTiAXbVnCRopZ-LObAqKnT-9rv-aySc5\n",
        "!ls"
      ],
      "metadata": {
        "id": "-hhNZp3s_lFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download dataset for training\n",
        "%cd ../..\n",
        "!ls\n",
        "!python download_datasets.py"
      ],
      "metadata": {
        "id": "-IIxFWdn7ocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Start the Genenration!\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path demodataset --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "aMSFMTb5UOE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define images for evaluation\n",
        "import os\n",
        "# define images for evaluation\n",
        "root_dataset = \"/content/EVA3D/datasets/DeepFashion/images/\"\n",
        "\n",
        "testing_images= [\"MEN-Jackets_Vests-id_00005346-01_4_full.png\",\n",
        "                 \"WOMEN-Dresses-id_00006993-03_7_additional.png\",\n",
        "                 \"WOMEN-Blouses_Shirts-id_00001722-07_7_additional.png\"]\n",
        "\n",
        "\n",
        "img_testing1 = os.path.join(root_dataset, testing_images[0])\n",
        "img_testing2 = os.path.join(root_dataset, testing_images[1])\n",
        "img_testing3 = os.path.join(root_dataset, testing_images[2])"
      ],
      "metadata": {
        "id": "0CKkRC9iCmvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define code for transformation\n",
        "\n",
        "\n",
        "# Run transformations\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QAZTzN7HCq1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Translation\n",
        "# Translation\n",
        "image = cv2.imread(img_testing1)\n",
        "ancho = image.shape[1] #columnas\n",
        "alto = image.shape[0] # filas\n",
        "\n",
        "M = np.float32([[1, 0,200],\n",
        "                [0, 1,150]])\n",
        "imageOut = cv2.warpAffine(image,M,(ancho,alto))\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "cv2.waitKey(0)\n",
        "output_img_path = img_testing1[:-4]+\"__translation.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "KeQQjk4oRnzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Translation\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "ys1OzozJXkfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Rotation\n",
        "# Rotation\n",
        "\n",
        "image = cv2.imread(img_testing2)\n",
        "ancho = image.shape[1] #columnas\n",
        "alto = image.shape[0] # filas\n",
        "# Rotación\n",
        "M = cv2.getRotationMatrix2D((ancho//2,alto//2),15,1)\n",
        "imageOut = cv2.warpAffine(image,M,(ancho,alto))\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "\n",
        "output_img_path = img_testing2[:-4]+\"__rotation.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "BJFpKzLZSRwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Rotation\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "0YHXRC0hSwa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformation: Scale\n",
        "image = cv2.imread(img_testing3)\n",
        "# Scale\n",
        "imageOut = cv2.resize(image,(600,300), interpolation=cv2.INTER_CUBIC)\n",
        "print('Input Image')\n",
        "cv2_imshow(image)\n",
        "print('Output Image')\n",
        "cv2_imshow(imageOut)\n",
        "output_img_path = img_testing3[:-4]+\"__scale.png\"\n",
        "print(output_img_path)\n",
        "cv2.imwrite(output_img_path, imageOut)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# save basename image to file for inference\n",
        "with open(\"file_test1.txt\", \"w\") as ft1:\n",
        "  ft1.write(os.path.basename(output_img_path))"
      ],
      "metadata": {
        "id": "gM3HbyclTueR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference: Scale\n",
        "!python generation_demo.py --batch 1 --chunk 1 --expname 512x256_deepfashion --dataset_path /content/EVA3D/datasets/DeepFashion --depth 5 --width 128 --style_dim 128 --renderer_spatial_output_dim 512 256 --input_ch_views 3 --white_bg --voxhuman_name eva3d_deepfashion --deltasdf --N_samples 28 --ckpt 420000 --identities 1 --truncation_ratio 0.5 --testing_list /content/EVA3D/file_test1.txt\n",
        "from IPython.display import Image\n",
        "Image('evaluations/512x256_deepfashion/iter_0420000/random_angles/images_paper_fig/0000000.png')"
      ],
      "metadata": {
        "id": "EUMnipDVXKzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}